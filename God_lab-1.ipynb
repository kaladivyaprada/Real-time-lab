{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d67d989a-ac70-445d-9011-9bf84033686c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: findspark in /home/hp/.local/lib/python3.10/site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fe1c39d-3c37-4d61-bf8e-38b3fc75043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d9f3a2-29ca-40e0-9557-850cf3e8ac60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.213.95:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>JupyterPySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7b567c58e6e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81216c10-1203-438e-99fb-b55569dae2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"JupyterPySpark\").getOrCreate()\n",
    "\n",
    "# Check Spark version\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16d2a405-ca7d-4c12-96f3-81291ae6664f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8d0cb79-f4ee-4ef4-b675-f0dc3482d85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+\n",
      "| id|   name|department|salary|\n",
      "+---+-------+----------+------+\n",
      "|  1|  Alice|        HR| 45000|\n",
      "|  2|    Bob|        IT| 60000|\n",
      "|  3|Charlie|        IT| 55000|\n",
      "|  4|  Diana|        HR| 48000|\n",
      "|  5| Edward|   Finance| 52000|\n",
      "|  6|  Fiona|        IT| 61000|\n",
      "+---+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv = spark.read.csv(\n",
    "   \"file:///home/hp/1SI24AD401/Lab program-1/data.csv\",  # path to your CSV file\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "df_csv.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92622111-3f8d-49e3-92ec-d3b1296a62c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/29 15:22:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----+----------+------------------+\n",
      "|summary|                id| name|department|            salary|\n",
      "+-------+------------------+-----+----------+------------------+\n",
      "|  count|                 6|    6|         6|                 6|\n",
      "|   mean|               3.5| NULL|      NULL|           53500.0|\n",
      "| stddev|1.8708286933869707| NULL|      NULL|6410.9281699298435|\n",
      "|    min|                 1|Alice|   Finance|             45000|\n",
      "|    max|                 6|Fiona|        IT|             61000|\n",
      "+-------+------------------+-----+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c55c2841-79fa-42bd-aa9b-831f73c39ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Total rows:\", df_csv.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29f4d306-6f83-4577-a544-4517075f3503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique departments: 3\n"
     ]
    }
   ],
   "source": [
    "unique_departments = df_csv.select(\"department\").distinct().count()\n",
    "print(\"Unique departments:\", unique_departments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3b6458e-5670-4cd9-aac8-bc97321407ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+\n",
      "| id|   name|department|salary|\n",
      "+---+-------+----------+------+\n",
      "|  1|  Alice|        HR| 45000|\n",
      "|  2|    Bob|        IT| 62000|\n",
      "|  3|Charlie|        IT| 55000|\n",
      "|  4|  Diana|        HR| 48000|\n",
      "|  5| Edward|   Finance| 52000|\n",
      "|  6|  Fiona|        IT| 61000|\n",
      "+---+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv_updated = df_csv.withColumn(\n",
    "    \"salary\",\n",
    "    when(col(\"name\") == \"Bob\", 62000).otherwise(col(\"salary\"))\n",
    ")\n",
    "df_csv_updated.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b831557-c9d2-4fb1-a687-8f56e414c1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cb33cc-38b3-42dd-9a09-a18364b5a871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23a8be55-82de-4f54-a510-3aae2dcf51b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 2: JSON Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a12443bf-76b0-437d-bb2d-166e98c14b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-------+------+\n",
      "|department| id|   name|salary|\n",
      "+----------+---+-------+------+\n",
      "|        HR|  1|  Alice| 45000|\n",
      "|        IT|  2|    Bob| 60000|\n",
      "|        IT|  3|Charlie| 55000|\n",
      "|        HR|  4|  Diana| 48000|\n",
      "|   Finance|  5| Edward| 52000|\n",
      "|        IT|  6|  Fiona| 61000|\n",
      "+----------+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json = spark.read.json(\"file:///home/hp/1SI24AD401/Lab program-1/data.json\")  # path to your JSON file\n",
    "df_json.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "638d80ea-e8c7-4b21-802a-b456ace7f9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Total records:\", df_json.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31626404-09b9-4e44-ac1e-843edab92303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+------+\n",
      "|department|    id|   name|salary|\n",
      "+----------+------+-------+------+\n",
      "|        HR|     1|  Alice| 45000|\n",
      "|        IT|     2|    Bob| 60000|\n",
      "|        IT|     3|Charlie| 55000|\n",
      "|        HR|     4|  Diana| 48000|\n",
      "|   Finance|     5| Edward| 52000|\n",
      "|        IT|     6|  Fiona| 61000|\n",
      "|         7|George|Finance| 53000|\n",
      "+----------+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data = [(7, \"George\", \"Finance\", 53000)]\n",
    "new_df = spark.createDataFrame(new_data, df_json.columns)\n",
    "\n",
    "df_json_new = df_json.union(new_df)\n",
    "df_json_new.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f02d833c-0da4-48c7-bb21-fc9c8ece31ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+------+\n",
      "|department|    id|   name|salary|\n",
      "+----------+------+-------+------+\n",
      "|        HR|     1|  Alice| 47000|\n",
      "|        IT|     2|    Bob| 60000|\n",
      "|        IT|     3|Charlie| 55000|\n",
      "|        HR|     4|  Diana| 48000|\n",
      "|   Finance|     5| Edward| 52000|\n",
      "|        IT|     6|  Fiona| 61000|\n",
      "|         7|George|Finance| 53000|\n",
      "+----------+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json_updated = df_json_new.withColumn(\n",
    "    \"salary\",\n",
    "    when(col(\"name\") == \"Alice\", 47000).otherwise(col(\"salary\"))\n",
    ")\n",
    "df_json_updated.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88bb5f7-683d-4e4f-8d2f-135c921008f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7adf90b5-4ee6-4da8-8cbf-0e614889a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").json(\n",
    "    \"file:///home/hp/1SI24AD401/Lab program-1/output_json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d564c-0626-4767-aa16-57c8745fdd5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
